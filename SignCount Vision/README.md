# SignCount Vision: Deep Learning for Sign Language Counting

This project utilizes several Deep Learning models to recognize and count hand signs in Sign Language. The models included in this project are VGG-16/19, ResNet-50/101/153, DenseNet-169/201, MobileNet, and InceptionV2. The models were trained on an image dataset specifically curated for counting in Sign Language.

## Table of Contents

- [Overview](#overview)
- [Models Used](#models-used)
- [Dataset](#dataset)
- [Contributing](#contributing)

## Overview

The goal of this project is to develop and compare various deep learning models for their effectiveness in recognizing and counting hand signs in Sign Language. This can be a significant step towards aiding communication for the hearing impaired.

## Models Used

The following deep learning models were used in this project:

- VGG-16
- VGG-19
- ResNet-50
- ResNet-101
- ResNet-153
- DenseNet-169
- DenseNet-201
- MobileNet
- InceptionV2

## Dataset

The dataset used for training the models consists of images representing different counting gestures in Sign Language. Each image is labeled with the corresponding count it represents. The dataset is split into training, validation, and test sets to evaluate the performance of the models.

## Contributing

Contributions are welcome! If you have any suggestions or improvements, please open an issue or submit a pull request.
